{"componentChunkName":"component---src-templates-blog-post-js","path":"/install-hortonworks-data-platform-hdp-3.1.5/","webpackCompilationHash":"d1ac6d0086ec5da93363","result":{"data":{"site":{"siteMetadata":{"title":"tech-blog","author":"Alamgir Qazi"}},"markdownRemark":{"id":"b1207a87-e71b-552e-93c8-50c761ca5c36","excerpt":"There are different way to install Hadoop ecosystem. One way is to install every component of Hadoop itself as they’re all open-source and…","html":"<p>\n  <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/tech-blog/static/0cfe2b0c4f31e93e5426868dedb34dc4/8ff1e/hortonworks.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n  \n  <span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block;  max-width: 590px; margin-left: auto; margin-right: auto;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 75%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsSAAALEgHS3X78AAACy0lEQVQ4y52QW0gUYRTHv9mdm66uhSSKJWUPIUmB5m1n19257uxlbDVL8VLKsgll9BwEEmWERuXmLShI15UV3dxLktZDT0EP1YPYU/QUhNRL9FYE05mZddvEgvrgt+f/nXP+5zuzCP3jufA0lI25+vyz4N+NZ9f69Rha60NnnvRm80LMjxqiAvImA5gz4cWbEx6sIlav1xzLHvhZlFH7yik90ZJqQ2JSQTWPOSQOScgx78aalzymxrhoBjOm9djuC6btjzuXjVrjnAwDF2Rj+qzbZJsV9WYwm5rGOWy7UXtgSwfS7UeklHLJnnBX5g7Vjz2nMffUxvkiTzKgcEn/VSZjhIf7mHlpQ4wrr/jVFtWxLAMeNvPJpuww27xYy8f911wJ30t7WmaZKcHJzEhf2Zj3M6cZH8kvmsb5amZS+Oaa86rND6Qfjoj7iyPmfudYkm/qiy3IZi1iVYM1tG2Me89FvKp9RlKZmDTSdIudcIZF1T7Gb9qnhQ9MRBpunORamevsBjPk3GRG2EVmnP/OTPCDzEPx9NHLDYXZT3uubuKusHSPu+0OshHvaGC95yAX9V1kbwijYlRhXDOesHultc73urPaNS3FhGGh3/+2+4DW64p4Bjo/hUpUVTX+NjNJHifyaZel1NpnJkiFLi4YIPLorrySwiC929IDdx+1y3KSsua3kgV5fkt5URdtze+1lFk7CAsdBJ+CIVzAMPwETlN1CKeoNzA0asKJO6BXYWjKTJHnzDhxBfQk1NLAOjAFfIR8BOJdqGl5rT8NvrAG3LsRTlIdcJFgehCiAtEL0QZbHobYDwiZnANog3x9Js8C2laC7iOpEMRjiKDo/UAlcAjYm6EKoIFyoIwgaQuwD3QxUAFouhAo1WuGdw/oYvTfhzICRuG/52FVDKZjMF3bdgssqyGPUwbEDoD/Vz/oHZuMxi1N/WmIobfVfwLjRMfC3R93ewAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n    >\n      <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\"\n        alt=\"Install multi-node Hadoop using Hortonworks Data Platform (HDP) 3.1.5 on Ubuntu 18 LTS\"\n        title=\"\"\n        src=\"/tech-blog/static/0cfe2b0c4f31e93e5426868dedb34dc4/b9e4f/hortonworks.png\"\n        srcset=\"/tech-blog/static/0cfe2b0c4f31e93e5426868dedb34dc4/cf440/hortonworks.png 148w,\n/tech-blog/static/0cfe2b0c4f31e93e5426868dedb34dc4/d2d38/hortonworks.png 295w,\n/tech-blog/static/0cfe2b0c4f31e93e5426868dedb34dc4/b9e4f/hortonworks.png 590w,\n/tech-blog/static/0cfe2b0c4f31e93e5426868dedb34dc4/8ff1e/hortonworks.png 800w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n      />\n    </span>\n  </span>\n  \n  </a>\n    </p>\n<p>There are different way to install Hadoop ecosystem. One way is to install every component of Hadoop itself as they’re all open-source and try to get them to work together. This is hard and also tiring. Other way is to use Hadoop Distributions like MapR, Cloudera Platform and Hortonworks Data Platform.</p>\n<p>I have some bad news. Cloudera purchased Hortonworks Data Platform and stopped its development. Now even if you want to install previously open-sourced Hortonworks Data Platform, you need to sign up with Cloudera and purchase it. That really sucks I know. </p>\n<p>There is still something you can still if you’re hell bent on installing Hadoop Ecosystem. What you do is pick the last stable Hortonworks Data Platform release before it got closed and you set that up. You get Ambari, plus can add as many nodes as you want. Pretty cool right. So this is what I’ll be going to do.</p>\n<p>First thing’s first, you need the Hortonworks Data Platform resource files. They are like 10 GB in size. You need to download them and put them in one of your machines (I’m assuming since you’re installing Hadoop, you have like 2,3 nodes/machines available). You can easily find the link from torrent but if you still can find it, hit me up. </p>\n<p>Here is what those resource files look like.</p>\n<p>\n  <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/tech-blog/static/6dea4ab03d3c86a9e1d3a916d044a0ba/331c8/Hortonworks-resource-files.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n  \n  <span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block;  max-width: 590px; margin-left: auto; margin-right: auto;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 27.157001414427157%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAAy0lEQVQY052P23LCMAwF+f9fZKCUpE2LcZImJr7GWylToM+cscbykbS2d+7mmSTKWokpEyRyWalAzIXhx2GHmSS5+pNbtl5ZuCXgY97Oqloru8UHDscTp/eG49uZ/f6w7QqOecXYnm9z5dqPAqxcjN1qqwAmd6P96ATsn0DvPW3T0knBXAzd5xfNucFayyo3j+NI3/dM8yyvqgzDSPn7gWoWP4TwBJZSUKiGFpxzOGnSszaklIgxknPWEZLk6t+lvjIeQF6UDv8H3/NfXXeFx9w0CKAAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n    >\n      <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\"\n        alt=\"Hortonworks resource files\"\n        title=\"\"\n        src=\"/tech-blog/static/6dea4ab03d3c86a9e1d3a916d044a0ba/b9e4f/Hortonworks-resource-files.png\"\n        srcset=\"/tech-blog/static/6dea4ab03d3c86a9e1d3a916d044a0ba/cf440/Hortonworks-resource-files.png 148w,\n/tech-blog/static/6dea4ab03d3c86a9e1d3a916d044a0ba/d2d38/Hortonworks-resource-files.png 295w,\n/tech-blog/static/6dea4ab03d3c86a9e1d3a916d044a0ba/b9e4f/Hortonworks-resource-files.png 590w,\n/tech-blog/static/6dea4ab03d3c86a9e1d3a916d044a0ba/331c8/Hortonworks-resource-files.png 707w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n      />\n    </span>\n  </span>\n  \n  </a>\n    </p>\n<p>You will need these four files. </p>\n<p>Step 1: </p>\n<p>copy these files to one of your ubuntu 18 machines. </p>\n<p>install nginx</p>\n<p>sudo apt install nginx</p>\n<p>copy these 4 files files to any location e.g /home/ta/www</p>\n<p>unzip these files using <code class=\"language-text\">tar -xvf filename.tar.gz</code></p>\n<p>do this for all 4 files</p>\n<p>Step 2: </p>\n<p>change nginx configurations</p>\n<p>nano /etc/nginx/sites-enabled/default</p>\n<p><img src=\"./nginx-config.png\" alt=\"nginx configuration\"></p>\n<p>update the root directory and make sure to bable autoIndex on in nginx</p>\n<p>once changes are done</p>\n<p><code class=\"language-text\">systemctl restart nginx</code></p>\n<p>check your IP and access it from the web</p>\n<p>\n  <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/tech-blog/static/993784833f2efd925446661b3d8a66fe/57a85/nginx-view.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n  \n  <span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block;  max-width: 590px; margin-left: auto; margin-right: auto;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 23.47641759406465%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAAeklEQVQY062N6wrCMAxG8/7vWdDFtbWlF3v7XDM2/zkEAyckH4eEQkzwPoBXA2M9tHVYzVPQ5jN/Y3oPbXG7Myi/AoCOMQZqrWgbR81s69eIt/u0LB3WdgmUUmBmpJSRcxah9/3ZFYdHzA3ONVlijHJoUko5xV8g/LneNgqIePYs11YAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n    >\n      <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\"\n        alt=\"nginx view\"\n        title=\"\"\n        src=\"/tech-blog/static/993784833f2efd925446661b3d8a66fe/b9e4f/nginx-view.png\"\n        srcset=\"/tech-blog/static/993784833f2efd925446661b3d8a66fe/cf440/nginx-view.png 148w,\n/tech-blog/static/993784833f2efd925446661b3d8a66fe/d2d38/nginx-view.png 295w,\n/tech-blog/static/993784833f2efd925446661b3d8a66fe/b9e4f/nginx-view.png 590w,\n/tech-blog/static/993784833f2efd925446661b3d8a66fe/f9b6a/nginx-view.png 885w,\n/tech-blog/static/993784833f2efd925446661b3d8a66fe/2d849/nginx-view.png 1180w,\n/tech-blog/static/993784833f2efd925446661b3d8a66fe/57a85/nginx-view.png 1887w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n      />\n    </span>\n  </span>\n  \n  </a>\n    </p>\n<p>All good. </p>\n<p>Now lets first discuss how our architecture will happen.</p>\n<p>Lets say you have 4 nodes/machines/VMs</p>\n<p>In the first machine, I would install Ambari (ambari-server). This is the GUI tool which will help us install different Hadoop components from the UI and update configurations. This is the primary reason why we decided for all this headache because eventually it will make our life easier. </p>\n<p>In the next 3 nodes, I will install Ambari-agent. This will help communicate b/w these nodes.</p>\n<p>Lets start on Node 01 by installing Ambari-server</p>\n<p><code class=\"language-text\">nano /etc/apt/sources.list.d/ambari.list</code></p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">deb http://10.2.4.68/ambari/ubuntu18/2.7.5.0-72/ Ambari main</code></pre></div>\n<p><code class=\"language-text\">nano /etc/apt/sources.list.d/ambari-hdp.list</code></p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">deb http://10.2.4.68/HDP-GPL/ubuntu18/3.1.5.0-152/ HDP-GPL main\ndeb http://10.2.4.68/HDP-UTILS/ubuntu18/1.1.0.22/ HDP-UTILS main\ndeb http://10.2.4.68/HDP/ubuntu18/3.1.5.0-152/ HDP main</code></pre></div>\n<p><code class=\"language-text\">sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys B9733A7A07513CAD</code></p>\n<p><code class=\"language-text\">sudo apt-get install ntp</code></p>\n<p><code class=\"language-text\">update-rc.d ntp defaults</code></p>\n<p>We will need to do these steps for all 4 nodes because we need them to access Hadoop installations and they can be accessed from this file server we created. </p>\n<p>now, to install <strong>ambari-server</strong></p>\n<p><code class=\"language-text\">apt-get install ambari-server</code></p>\n<p>We need to update msql connector as it creates issues for us</p>\n<p><code class=\"language-text\">wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-8.0.25.tar.gz</code></p>\n<p>move mysql-connector-java-8.0.25.jar to /var/lib/ambari-server/resources/ and rename to mysql-connector-java.jar</p>\n<p>Disable Firewall:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">sudo ufw disable\nsudo iptables -X\nsudo iptables -t nat -F\nsudo iptables -t nat -X\nsudo iptables -t mangle -F\nsudo iptables -t mangle -X\nsudo iptables -P INPUT ACCEPT\nsudo iptables -P FORWARD ACCEPT\nsudo iptables -P OUTPUT ACCEPT</code></pre></div>\n<p>update your hostname: </p>\n<p><code class=\"language-text\">nano /etc/hostname</code>\n<code class=\"language-text\">ambarimagent.com</code></p>\n<p>update yours hosts: </p>\n<p>since you will add 3 more ambari-agents</p>\n<p><code class=\"language-text\">nano /etc/hosts</code></p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">10.2.4.69   ambariserver.com\n10.2.4.70   ambariagent1.com\n10.2.4.71   ambariagent2.com\n10.2.4.72   ambariagent3.com</code></pre></div>\n<p>Now lets set up SSH keys: </p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">ssh localhost\nssh-keygen -t rsa -P &#39;&#39; -f ~/.ssh/id_rsa\ncat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys\nchmod 0600 ~/.ssh/authorized_keys</code></pre></div>\n<p>We will need this as we’ll need to pass these keys b/w machines to enable paswordless SSH.</p>\n<p>Now lets setup ambari-server via</p>\n<p><code class=\"language-text\">ambari-server setup</code></p>\n<p>Once done, open up 10.2.4.69:8080 and ambari UI should load up. Login via admin/admin</p>\n<p>To allow adding other nodes to ambari-server, you need to pass ID_RSA Keys from ambari server to other nodes</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">ssh-copy-id -i ~/.ssh/id_rsa.pub root@ambariagent1.com</code></pre></div>\n<p>now do this same for all agents.</p>\n<p>Once done, you need to do the same from all the agent machines.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">ssh-copy-id -i ~/.ssh/id_rsa.pub root@ambariserver.com</code></pre></div>\n<p>Lets setup <em>ambari-agent</em></p>\n<p><code class=\"language-text\">apt-get install ambari-agent</code> (make sure you followed the steps we did for ambari-server)</p>\n<p>There is one more step we need to do for every ambari-agent</p>\n<p><code class=\"language-text\">sudo nano /etc/ambari-agent/conf/ambari-agent.ini</code></p>\n<p>change hostname to ambariserver.com</p>\n<p>that’s it. Ambari-server and Ambari-agent have been setup.</p>\n<p>Now just go to Ambari UI, add each ambari-agent node one by one. </p>\n<p>you will need to write their hostnames and their id<em>rsa keys in the below boxes which u can copy/paste from `cat .ssh/id</em>rsa`</p>\n<p>This was kinda simplified way of installing Hadoop Hortonworks Data Platform. Hadoop is a huge ecosystem, make sure you know what your use-cases are for using it. If you want big-data analytics, Hadoop Ecosystem might not be the best toolset anymore. If you do lots of ETL and processing in memory, yes Hadoop is great and will help you out. Moreover, best bits of Hadoop (Spark Ecosystem) are not dependant on Hadoop anymore which is great. Bottomline is, Hadoop is great but understand your use case. Experiment with this whether it will work for you or not. </p>\n<p>We had a use-case where we processed TBs of data (per day) and needed real-time statistics. Hadoop failed for this. Watch out this space and I will share how we handled that.</p>","frontmatter":{"title":"Install Multi-node Hadoop using Hortonworks Data Platform (HDP) 3.1.5 on Ubuntu 18 LTS","date":"July 18, 2021"}}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"/install-hortonworks-data-platform-hdp-3.1.5/","previous":{"fields":{"slug":"/docker-microservices/"},"frontmatter":{"title":"Docker and Microservices"}},"next":null}}}